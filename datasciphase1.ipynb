{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoudySaleh-IT/CSC212Project/blob/master/datasciphase1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Imports & Logging\n",
        "import time, json, csv, re, sys, traceback\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "from datetime import datetime\n",
        "\n",
        "def log(msg, level=\"INFO\"):\n",
        "    print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}][{level}] {msg}\")"
      ],
      "metadata": {
        "id": "Lsp1MjHXMKym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Config\n",
        "BASE = \"https://kitchen.sayidaty.net\"\n",
        "START_LIST = \"https://kitchen.sayidaty.net/recipes/index/cuisine/2419\"  # المطبخ السعودي\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (compatible; research-bot/1.0; +https://example.edu)\"\n",
        "}\n",
        "REQUEST_TIMEOUT = 20\n",
        "SLEEP_BETWEEN_REQUESTS = 1.5\n",
        "MAX_PAGES = 12  # زِدها إذا تحتاج صفحات أكثر\n",
        "\n",
        "# مخرجات\n",
        "OUT_JSONL = \"sayidaty_saudi_recipes.jsonl\"\n",
        "OUT_CSV   = \"sayidaty_saudi_recipes.csv\"\n",
        "URLS_CHECKPOINT = \"sayidaty_urls.txt\""
      ],
      "metadata": {
        "id": "DzG8VTt0MV8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: HTTP helper\n",
        "def get_soup(url):\n",
        "    try:\n",
        "        log(f\"GET {url}\")\n",
        "        r = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT)\n",
        "        r.raise_for_status()\n",
        "        return BeautifulSoup(r.text, \"html.parser\")\n",
        "    except requests.HTTPError as e:\n",
        "        log(f\"HTTPError {url}: {e}\", \"ERROR\")\n",
        "        raise\n",
        "    except requests.RequestException as e:\n",
        "        log(f\"RequestException {url}: {e}\", \"ERROR\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        log(f\"Unknown error while fetching {url}: {e}\", \"ERROR\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "KAHxo3CuMTYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Listing page -> recipe URLs\n",
        "def list_page_urls(list_url):\n",
        "    try:\n",
        "        soup = get_soup(list_url)\n",
        "        links = []\n",
        "        # فلتر روابط /node/ لأنها غالبًا للوصفات\n",
        "        for a in soup.select(\"a[href*='/node/']\"):\n",
        "            href = a.get(\"href\")\n",
        "            if href and \"/node/\" in href:\n",
        "                links.append(urljoin(BASE, href))\n",
        "        unique_links = sorted(set(links))\n",
        "        log(f\"Found {len(unique_links)} recipe links in listing page\")\n",
        "        return unique_links\n",
        "    except Exception:\n",
        "        log(\"Failed to parse listing page; see traceback below\", \"ERROR\")\n",
        "        traceback.print_exc()\n",
        "        return []"
      ],
      "metadata": {
        "id": "qBP4aGxDMeXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Paginate listing\n",
        "def paginate_listing(start_url, max_pages=12):\n",
        "    all_urls = []\n",
        "    for p in range(1, max_pages + 1):\n",
        "        page_url = start_url if p == 1 else f\"{start_url}/?page={p}\"\n",
        "        log(f\"Listing page {p} -> {page_url}\")\n",
        "        try:\n",
        "            urls = list_page_urls(page_url)\n",
        "            if not urls:\n",
        "                log(\"No more urls found, stopping pagination\")\n",
        "                break\n",
        "            all_urls.extend(urls)\n",
        "        except Exception:\n",
        "            log(\"Error while listing this page; continuing to next page\", \"ERROR\")\n",
        "            traceback.print_exc()\n",
        "        time.sleep(SLEEP_BETWEEN_REQUESTS)\n",
        "    all_urls = sorted(set(all_urls))\n",
        "    # تشيكبوينت للروابط لاستخدامه لو وقفت النصّاعة\n",
        "    try:\n",
        "        with open(URLS_CHECKPOINT, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"\\n\".join(all_urls))\n",
        "        log(f\"Saved URLs checkpoint: {URLS_CHECKPOINT} ({len(all_urls)} urls)\")\n",
        "    except Exception:\n",
        "        log(\"Failed to write URLs checkpoint\", \"ERROR\")\n",
        "        traceback.print_exc()\n",
        "    return all_urls"
      ],
      "metadata": {
        "id": "SPCVj__-MgKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Extract single recipe\n",
        "def extract_recipe(url):\n",
        "    try:\n",
        "        soup = get_soup(url)\n",
        "\n",
        "        # العنوان\n",
        "        title_el = soup.find([\"h1\", \"h2\"], string=re.compile(r\".+\"))\n",
        "        title = title_el.get_text(strip=True) if title_el else None\n",
        "\n",
        "        # التاريخ (تقريبي إن وُجد)\n",
        "        date_text = None\n",
        "        if title_el:\n",
        "            date_node = title_el.find_next(string=re.compile(r\"\\d{4}-\\d{2}-\\d{2}|\\d{4}\"))\n",
        "            date_text = (date_node.strip() if date_node else None)\n",
        "\n",
        "        # الوقت / يكفي ل\n",
        "        time_text = None; serves = None\n",
        "        time_label = soup.find([\"span\",\"div\"], string=re.compile(\"وقت الطه\"))\n",
        "        if time_label:\n",
        "            t = time_label.find_next()\n",
        "            time_text = t.get_text(strip=True) if t else None\n",
        "        serves_label = soup.find([\"span\",\"div\"], string=re.compile(\"يكفي ل\"))\n",
        "        if serves_label:\n",
        "            s = serves_label.find_next()\n",
        "            serves = s.get_text(strip=True) if s else None\n",
        "\n",
        "        # المقادير\n",
        "        ingredients = []\n",
        "        ing_header = soup.find([\"h3\",\"h2\"], string=re.compile(\"المقادير\"))\n",
        "        if ing_header:\n",
        "            ul = ing_header.find_next([\"ul\",\"ol\"])\n",
        "            if ul:\n",
        "                for li in ul.find_all(\"li\"):\n",
        "                    txt = li.get_text(\" \", strip=True)\n",
        "                    if txt: ingredients.append(txt)\n",
        "\n",
        "        # الخطوات\n",
        "        steps = []\n",
        "        steps_header = soup.find([\"h3\",\"h2\"], string=re.compile(\"طريقة التحضير\"))\n",
        "        if steps_header:\n",
        "            ol = steps_header.find_next([\"ol\",\"ul\"])\n",
        "            if ol:\n",
        "                for i, li in enumerate(ol.find_all(\"li\"), 1):\n",
        "                    txt = li.get_text(\" \", strip=True)\n",
        "                    if txt: steps.append(f\"{i}. {txt}\")\n",
        "\n",
        "        # الوسوم\n",
        "        tags = []\n",
        "        tags_header = soup.find([\"h5\",\"h4\",\"h3\"], string=re.compile(\"سمات\"))\n",
        "        if tags_header:\n",
        "            tag_container = tags_header.find_next()\n",
        "            if tag_container:\n",
        "                for a in tag_container.find_all(\"a\", href=True):\n",
        "                    t = a.get_text(strip=True)\n",
        "                    if t: tags.append(t)\n",
        "\n",
        "        # الصورة\n",
        "        img = None\n",
        "        img_el = soup.find(\"img\")\n",
        "        if img_el and img_el.get(\"src\"):\n",
        "            img = urljoin(BASE, img_el[\"src\"])\n",
        "\n",
        "        data = {\n",
        "            \"title\": title,\n",
        "            \"date\": date_text,\n",
        "            \"time\": time_text,\n",
        "            \"serves\": serves,\n",
        "            \"ingredients\": ingredients,\n",
        "            \"steps\": steps,\n",
        "            \"tags\": tags,\n",
        "            \"image\": img,\n",
        "            \"url\": url,\n",
        "        }\n",
        "        return data\n",
        "    except Exception:\n",
        "        log(f\"extract_recipe failed for: {url}\", \"ERROR\")\n",
        "        traceback.print_exc()\n",
        "        return None"
      ],
      "metadata": {
        "id": "YGXrwUWiMi_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Saudi filter\n",
        "SAUDI_HINTS = {\n",
        "    \"المطبخ السعودي\", \"طبخات سعودية\", \"وصفات سعودية\",\n",
        "    \"كبسة\", \"سليق\", \"ثريد\", \"مقلقل\", \"مندي\",\n",
        "    \"قرصان\", \"مرقوق\", \"جريش\", \"كليجة\", \"لقيمات\"\n",
        "}\n",
        "\n",
        "def is_saudi_recipe(data):\n",
        "    if not data:\n",
        "        return False\n",
        "    tags = set(data.get(\"tags\") or [])\n",
        "    # بما أننا داخل قائمة المطبخ السعودي، هذا فلتر إضافي فقط\n",
        "    return True if not tags else any(t in SAUDI_HINTS for t in tags)"
      ],
      "metadata": {
        "id": "FmeFcJ7rMlJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Crawl loop\n",
        "def crawl_all():\n",
        "    try:\n",
        "        urls = paginate_listing(START_LIST, max_pages=MAX_PAGES)\n",
        "        log(f\"Total URLs gathered: {len(urls)}\")\n",
        "    except Exception:\n",
        "        log(\"Failed during pagination stage\", \"ERROR\")\n",
        "        traceback.print_exc()\n",
        "        return []\n",
        "\n",
        "    out = []\n",
        "    for i, u in enumerate(urls, 1):\n",
        "        log(f\"[{i}/{len(urls)}] Extract -> {u}\")\n",
        "        try:\n",
        "            d = extract_recipe(u)\n",
        "            if d and is_saudi_recipe(d):\n",
        "                out.append(d)\n",
        "            else:\n",
        "                log(\"Skipped (not Saudi or extraction returned None)\", \"INFO\")\n",
        "        except Exception:\n",
        "            log(\"Unexpected error in crawl loop for this URL\", \"ERROR\")\n",
        "            traceback.print_exc()\n",
        "        time.sleep(SLEEP_BETWEEN_REQUESTS)\n",
        "\n",
        "    log(f\"Crawl finished. Valid recipes: {len(out)}\")\n",
        "    return out"
      ],
      "metadata": {
        "id": "AuAmDpK8Ml3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Exporters\n",
        "def export_jsonl(rows, path=OUT_JSONL):\n",
        "    try:\n",
        "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "            for r in rows:\n",
        "                f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "        log(f\"Saved JSONL -> {path}\")\n",
        "    except Exception:\n",
        "        log(\"Failed to write JSONL\", \"ERROR\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "def export_csv(rows, path=OUT_CSV):\n",
        "    try:\n",
        "        with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            w = csv.writer(f)\n",
        "            w.writerow([\"title\",\"date\",\"time\",\"serves\",\"ingredients\",\"steps\",\"tags\",\"image\",\"url\"])\n",
        "            for r in rows:\n",
        "                w.writerow([\n",
        "                    r.get(\"title\"),\n",
        "                    r.get(\"date\"),\n",
        "                    r.get(\"time\"),\n",
        "                    r.get(\"serves\"),\n",
        "                    \" | \".join(r.get(\"ingredients\",[])),\n",
        "                    \" | \".join(r.get(\"steps\",[])),\n",
        "                    \" | \".join(r.get(\"tags\",[])),\n",
        "                    r.get(\"image\"),\n",
        "                    r.get(\"url\"),\n",
        "                ])\n",
        "        log(f\"Saved CSV -> {path}\")\n",
        "    except Exception:\n",
        "        log(\"Failed to write CSV\", \"ERROR\")\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "id": "Dcd8izIfMoeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Main\n",
        "def main():\n",
        "    log(\"=== START SCRAPE ===\")\n",
        "    try:\n",
        "        rows = crawl_all()\n",
        "        export_jsonl(rows, OUT_JSONL)\n",
        "        export_csv(rows, OUT_CSV)\n",
        "        log(f\"=== DONE; Recipes: {len(rows)} ===\")\n",
        "    except KeyboardInterrupt:\n",
        "        log(\"Interrupted by user\", \"ERROR\")\n",
        "    except Exception:\n",
        "        log(\"Fatal error in main()\", \"ERROR\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "-TB5sBM8MrdP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}